{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "with open(\"names.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    words = text.splitlines()\n",
    "    chars = sorted(set(\"\".join(words)))\n",
    "    print(chars)\n",
    "\n",
    "stoi = {char:integer+1 for integer,char in enumerate(chars)}\n",
    "itos = {integer+1:char for integer,char in enumerate(chars)}\n",
    "\n",
    "stoi[\".\"] = 0\n",
    "itos[0] = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1], device='cuda:0')\n",
      "tensor([ 5, 13, 13,  1,  0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = [], []\n",
    "for w in words[:1]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for char1, char2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[char1]\n",
    "        ix2 = stoi[char2]\n",
    "\n",
    "        inputs.append(ix1) #Char before the label\n",
    "        labels.append(ix2) #Char expected after ix1\n",
    "\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "one_hot_inputs = F.one_hot(inputs, num_classes=27).type(torch.float32)\n",
    "print(one_hot_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0183],\n",
      "        [-0.8126],\n",
      "        [ 0.4512],\n",
      "        [ 0.4512],\n",
      "        [ 0.2537]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n5x27 @ 27x1 --> 5x1\\n@ --> pytorch matrix multiplication operator\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the first neuron\n",
    "W = torch.randn((27, 1)) #Each input has its own weight\n",
    "outputs = one_hot_inputs @ W\n",
    "print(outputs)\n",
    "\"\"\"\n",
    "5x27 --> 5 letters and there are 27 letters total\n",
    "5x27 @ 27x1 --> 5x1\n",
    "@ --> pytorch matrix multiplication operator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0202, 0.0130, 0.0241, 0.0224, 0.0346], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor(3.8258, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#All of this is basically a forward pass\n",
    "#Creating 27 neurons\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "logits = one_hot_inputs @ W\n",
    "\n",
    "\"\"\"\n",
    "5x27 @ 27x27 --> 5x27\n",
    "\"\"\"\n",
    "\n",
    "#These 2 (two) lines is basically a softmax activation function being applied to the layer\n",
    "counts = torch.exp(logits)\n",
    "probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "probs_of_guessing_correctly = probs[torch.arange(5), labels]\n",
    "print(probs_of_guessing_correctly) #probs of guessing each of the 5 letters correclty\n",
    "negative_log_likelihood_mean = -probs_of_guessing_correctly.log().mean()\n",
    "loss = negative_log_likelihood_mean\n",
    "print(negative_log_likelihood_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0311, 0.0372, 0.0345, 0.0398, 0.0349, 0.0431, 0.0359, 0.0421, 0.0396,\n",
      "         0.0399, 0.0340, 0.0376, 0.0311, 0.0387, 0.0328, 0.0439, 0.0435, 0.0331,\n",
      "         0.0369, 0.0407, 0.0400, 0.0309, 0.0328, 0.0421, 0.0375, 0.0356, 0.0309],\n",
      "        [0.0437, 0.0317, 0.0362, 0.0445, 0.0434, 0.0358, 0.0341, 0.0333, 0.0320,\n",
      "         0.0408, 0.0382, 0.0432, 0.0401, 0.0372, 0.0398, 0.0362, 0.0317, 0.0316,\n",
      "         0.0363, 0.0326, 0.0369, 0.0381, 0.0438, 0.0361, 0.0376, 0.0308, 0.0344],\n",
      "        [0.0368, 0.0416, 0.0389, 0.0330, 0.0391, 0.0415, 0.0430, 0.0328, 0.0319,\n",
      "         0.0383, 0.0343, 0.0361, 0.0350, 0.0427, 0.0438, 0.0419, 0.0418, 0.0381,\n",
      "         0.0378, 0.0333, 0.0338, 0.0330, 0.0416, 0.0309, 0.0315, 0.0339, 0.0337],\n",
      "        [0.0368, 0.0416, 0.0389, 0.0330, 0.0391, 0.0415, 0.0430, 0.0328, 0.0319,\n",
      "         0.0383, 0.0343, 0.0361, 0.0350, 0.0427, 0.0438, 0.0419, 0.0418, 0.0381,\n",
      "         0.0378, 0.0333, 0.0338, 0.0330, 0.0416, 0.0309, 0.0315, 0.0339, 0.0337],\n",
      "        [0.0345, 0.0343, 0.0358, 0.0331, 0.0329, 0.0348, 0.0375, 0.0343, 0.0368,\n",
      "         0.0383, 0.0315, 0.0330, 0.0408, 0.0315, 0.0367, 0.0428, 0.0408, 0.0359,\n",
      "         0.0352, 0.0387, 0.0417, 0.0322, 0.0449, 0.0376, 0.0418, 0.0400, 0.0428]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Other way of doing this\n",
    "\n",
    "class BigramNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_features=27, out_features=27, bias=False, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer1(inputs)\n",
    "\n",
    "model = BigramNN()\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = model(one_hot_inputs)\n",
    "    preds = torch.softmax(logits, dim=1)\n",
    "print(preds)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous loss --> 3.825788974761963\n",
      "current loss --> 3.8046321868896484\n"
     ]
    }
   ],
   "source": [
    "print(f\"previous loss --> {loss}\")\n",
    "#Backward propagation\n",
    "#Zero the grad\n",
    "W.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "#If a value on the grad is positivie it means that, that one value needs go down by tweaking our weights\n",
    "#likewise for negatives values\n",
    "#print(W.grad)\n",
    "\n",
    "W.data += -0.1*W.grad #simples gradient descent\n",
    "logits = one_hot_inputs @ W\n",
    "counts = torch.exp(logits)\n",
    "probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "probs_of_guessing_right = probs[torch.arange(5), labels]\n",
    "negative_log_likelihood_mean = -probs_of_guessing_right.log().mean()\n",
    "loss = negative_log_likelihood_mean\n",
    "\n",
    "\n",
    "print(f\"current loss --> {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop\n",
    "epochs = 10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
