{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build dataset\n",
    "\n",
    "with open(\"names.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    words = text.splitlines()\n",
    "    chars_used = sorted(set(\"\".join(words)))\n",
    "\n",
    "stoi = {char:integer+1 for integer,char in enumerate(chars_used)}\n",
    "itos = {integer+1:char for integer,char in enumerate(chars_used)}\n",
    "\n",
    "stoi[\".\"] = 0\n",
    "itos[0] = \".\"\n",
    "\n",
    "inputs, labels = [], []\n",
    "for w in words[:]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for char1, char2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[char1]\n",
    "        ix2 = stoi[char2]\n",
    "\n",
    "        inputs.append(ix1)\n",
    "        labels.append(ix2)\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "W = torch.randn((len(stoi), len(stoi)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 228146 examples\n",
      "Loss before --> 2.454224109649658\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')\n",
      "Loss after epochs --> 2.4542222023010254\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print(f\"Training on {inputs.nelement()} examples\")\n",
    "one_hot_inputs = F.one_hot(inputs, num_classes=len(stoi)).type(torch.float32)\n",
    "logits = one_hot_inputs @ W\n",
    "counts = torch.exp(logits)\n",
    "probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "loss = -probs[torch.arange(inputs.nelement()), labels].log().mean()\n",
    "print(f\"Loss before --> {loss}\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Forward pass\n",
    "    #One hot vectors are going through the neural network's neurons\n",
    "    logits = one_hot_inputs @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "    loss = -probs[torch.arange(inputs.nelement()), labels].log().mean()\n",
    "\n",
    "    #Zero the grad\n",
    "    W.grad = None\n",
    "\n",
    "    #Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    #//Optmizer.step\n",
    "    #the number is basically the learning rate\n",
    "    W.data += -1*W.grad\n",
    "\n",
    "logits = one_hot_inputs @ W\n",
    "counts = torch.exp(logits)\n",
    "probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "loss = -probs[torch.arange(inputs.nelement()), labels].log().mean()\n",
    "print(f\"Loss after epochs --> {loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eadvozeaieziaveedrtaed', 'mstolaniar', 'wi', 'shs', 'fhar', 'slwabbaiohniyaers', 'niiar', 'traeomr', 'knaekcehlsieeedr', 'niiear']\n"
     ]
    }
   ],
   "source": [
    "#Sampling from the model\n",
    "\n",
    "names = []\n",
    "n_names = 10\n",
    "ix = 0\n",
    "\n",
    "for n in range(n_names):\n",
    "    current_name = \"\"\n",
    "    while True:\n",
    "        logits = one_hot_inputs @ W\n",
    "        counts = torch.exp(logits)\n",
    "        probs = torch.div(counts, counts.sum(dim=1, keepdim=True))\n",
    "        p = probs[ix, :]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        if ix == 0:\n",
    "            names.append(current_name)\n",
    "            break\n",
    "        else:\n",
    "            current_name += itos[ix]\n",
    "\n",
    "print(names)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
