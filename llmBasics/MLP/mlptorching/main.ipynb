{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n",
      "\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "\n",
    "stoi = {char:integer for integer, char in enumerate(chars)}\n",
    "itos = {integer:char for integer, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda enc: [stoi[c] for c in enc]\n",
    "decode = lambda dec: \"\".join([itos[i] for i in dec])\n",
    "\n",
    "print(encode(\"\\n\"))\n",
    "print(decode(encode(\"\\n\")))\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m context \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m context_size\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m n \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     idx \u001b[39m=\u001b[39m stoi[char]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     inputs\u001b[39m.\u001b[39mappend(context)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     labels\u001b[39m.\u001b[39mappend(idx)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "context_size = 3\n",
    "for n in names[:]:\n",
    "    context = [0] * context_size\n",
    "    for char in n + \".\":\n",
    "        idx = stoi[char]\n",
    "        inputs.append(context)\n",
    "        labels.append(idx)\n",
    "        context = context[1:] + [idx]\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 2\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embtest \u001b[39m=\u001b[39m C[inputs]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(vocab_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/mlptorching/main.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "embtest = C[inputs]\n",
    "print(vocab_size)\n",
    "print(inputs.shape)\n",
    "print(embtest.shape)\n",
    "\n",
    "embtest = embtest.view(-1, context_size*n_embd)\n",
    "print(embtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super(MLP, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_emb_table = nn.Embedding(context_size, n_embd)\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, batched:bool):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        #print(f\"x.shape[1] --> {x.shape[1]}\")\n",
    "        pos_emb = self.positional_emb_table(torch.arange(x.shape[1])) # --> context_size X n_embd\n",
    "        #print(pos_emb.shape)\n",
    "        result_emb = tok_emb + pos_emb\n",
    "        #test = self.token_embedding(torch.tensor([0])).shape --> [2] #basically it's a 1 X 1 X 2 but squeezed\n",
    "        #test = self.token_embedding(torch.tensor([0, 2])).shape --> [2, 2]\n",
    "        #test = self.token_embedding(torch.tensor([0, 2, 2])).shape --> [3, 2] #the n_embd is always preserved so it can go into the lm_head\n",
    "        #print(test)\n",
    "        #emb = emb.view(-1, context_size*n_embd)\n",
    "        #OR --> B, T, C = emb.shape #batch --> how many inputs in the batch #Time --> context size #Channels --> n_embd\n",
    "        logits = self.lm_head(result_emb)\n",
    "        if batched:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #96 X 2 #assuming batch_size = 32 and n_embd = 2\n",
    "\n",
    "        #The explanation below is for a batch size of 32 and vocab size of 27\n",
    "        #logits = self.lm_head(emb) # turns this into --> 96 X 27 #so there must be 96 labels in total to be compared those 96 inputs #because of B * T in the view above it's not 32 inputs of 3 characters anymore, it's now 96 characters with 27 probabilities each\n",
    "        #if the input is not bached, then there is no batch input dimension nor time, the only output plucked out of the emb table are 2 channels, the shape of it being [2] ex --> [-2.243, 0.763]\n",
    "\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generator(self, starting_idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            #max_new_tokens is needed since there is no special character to stop the generation\n",
    "            logits = self(starting_idx, False) #plucks out the 0th row of the token embedding table\n",
    "            #logits are of shape --> B*T X vocab_size or just batch_size * n_context_character X vocab_size\n",
    "\n",
    "            #then based on the last character, predict the next one\n",
    "\n",
    "            #print(logits.shape) #since it's being passed as batched, there is no batch_dimension in the logits\n",
    "            #but if it's passed as unbatched, there will be an extra dimension at the beggining of logits\n",
    "            #print(logits[:, -1, :]) # --> no need for the last : but just in case it can be useful\n",
    "            #if it's passed as unbatched, there will be an extra dimension at the beggining, thus the need to do it like this --> [:, -1, :]\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=1) #no dim passed because is unbatched, if batched, pass dim as 1\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            starting_idx = torch.cat((starting_idx, new_idx), dim=1)\n",
    "            #print(starting_idx)\n",
    "        return starting_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, context_size):\n",
    "    batch_idx = torch.randint(0, data.shape[0]-context_size, (batch_size,))\n",
    "    x = torch.stack([data[idx:idx+context_size] for idx in batch_idx])\n",
    "    y = torch.stack([data[idx+1:idx+1+context_size] for idx in batch_idx])\n",
    "\n",
    "    #decode x to view what it is\n",
    "    #print(x[0].tolist())\n",
    "    #print(decode(x[0].tolist()))\n",
    "    #print(decode(y[0].tolist()))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "x, y = get_batch(data, 32, 3)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "context_size = 256 #also max context for predictions\n",
    "batch_size = 64\n",
    "\n",
    "model = MLP(vocab_size=vocab_size, n_embd=n_embd)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:15<00:00, 132.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4551, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    x, y = get_batch(data, batch_size, context_size)\n",
    "    #print(x.shape)\n",
    "    logits = model(x, True)\n",
    "    #print(y.shape)\n",
    "    #print(logits.shape)\n",
    "    #print(y)\n",
    "    loss = loss_fn(logits, y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SThind bilowenser the wintthig st, d hearour ort ne nd pu Bulathathyor hayowitwn I wim.\n",
      "Tlleile asome,\n",
      "Y t f heasitougo caolle top my CAUBut desus han?\n",
      "XENG:\n",
      "NG:\n",
      "\n",
      "Thoure f kired dingoomaragucetheendsin\n",
      "Myt s.\n",
      "nowngir m liond tbinak:\n",
      "Yo opr--d the tl\n",
      "Thain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#starts with a new line\n",
    "with torch.inference_mode():\n",
    "    sample = model.generator(torch.tensor([[0]]), 256)\n",
    "    sample_tolist = sample[0].tolist()\n",
    "    decoded = decode(sample_tolist)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
