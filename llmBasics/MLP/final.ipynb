{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device set to cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"Default device set to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"names.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    words = text.splitlines()\n",
    "    chars = sorted(set((\"\".join(words))))\n",
    "\n",
    "stoi = {char:integer+1 for integer,char in enumerate(chars)}\n",
    "itos = {integer+1:char for integer,char in enumerate(chars)}\n",
    "\n",
    "stoi[\".\"] = 0\n",
    "itos[0] = \".\"\n",
    "\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = [], []\n",
    "context_size = 3 #Block size\n",
    "\n",
    "for w in words[:]:\n",
    "    context = [0] * context_size\n",
    "    for char in w + \".\":\n",
    "        ix = stoi[char]\n",
    "        inputs.append(context)\n",
    "        labels.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1948, -1.1598],\n",
      "         [-1.1948, -1.1598],\n",
      "         [-1.1948, -1.1598]]], device='cuda:0')\n",
      "tensor([[-1.1948, -1.1598, -1.1948, -1.1598, -1.1948, -1.1598]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embedding = torch.randn((vocab_size, 2))\n",
    "embedded = embedding[inputs]\n",
    "print(embedded[:1])\n",
    "embedded_feedforward = embedded.view(embedded.shape[0], context_size*2)\n",
    "print(embedded_feedforward[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defning params for forward pass\n",
    "\n",
    "# Inputs @ Weights + biases\n",
    "# m X n    n X p\n",
    "\n",
    "# Result --> embedded_feedforward.shape[1] X p\n",
    "\n",
    "w1_n_neurons = 2000\n",
    "w2_n_neurons = vocab_size #27 different characters to be predicted\n",
    "\n",
    "W1 = torch.randn((embedded_feedforward.shape[1], w1_n_neurons))\n",
    "b1 = torch.randn((w1_n_neurons))\n",
    "\n",
    "# Hidden Layer @ W2 + biases2\n",
    "# n X p            p X vocab_size\n",
    "\n",
    "W2 = torch.randn((w1_n_neurons, w2_n_neurons))\n",
    "b2 = torch.randn((w2_n_neurons))\n",
    "\n",
    "params = [embedding, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:34<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6537, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = -0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "\n",
    "    embedded_feedforward = embedding[inputs].view(embedded.shape[0], context_size*2)\n",
    "    hidden_layer = torch.tanh(embedded_feedforward @ W1 + b1)\n",
    "    logits = hidden_layer @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for param in params:\n",
    "        param.data += lr * param.grad\n",
    "\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "tensor([189255,  45137, 132600, 127339, 188739, 103533,  10022, 199029, 115773,\n",
      "        118999, 205292, 162118,  41799, 183687, 135551,   2220,  39997, 183070,\n",
      "        168301, 119452, 161958,  16972,  61822,  30158, 218652,  70642, 226070,\n",
      "        195489,  14589, 191483, 158296,  45816], device='cuda:0')\n",
      "tensor([ 2, 18,  9], device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor([18,  9, 14], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(torch.randint(0, inputs.shape[0], (32,))) #Random location for a batch\n",
    "\n",
    "print(inputs[2989]) #a random input for a batch\n",
    "print(labels[2989]) #a random label for a batch\n",
    "print(inputs[2990])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 57.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([183618,  90314, 118532, 208236, 223653,  18869, 132579,  14748,  65795,\n",
      "        129448,  78632,  24806,  61435, 209417,  86837,  66943,  53227, 173939,\n",
      "         14043, 151076,  26669,  63205, 167467,  34925, 110213,  88982, 207545,\n",
      "        103525, 178633, 215949, 109798, 221257], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (228146).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/evie/torching/llmBasics/MLP/final.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/final.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/final.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         param\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m param\u001b[39m.\u001b[39mgrad\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/final.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/llmBasics/MLP/final.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:3039\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m \n\u001b[1;32m   2975\u001b[0m \u001b[39mSee :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39m    >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target, weight):\n\u001b[0;32m-> 3039\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3040\u001b[0m         cross_entropy,\n\u001b[1;32m   3041\u001b[0m         (\u001b[39minput\u001b[39;49m, target, weight),\n\u001b[1;32m   3042\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3043\u001b[0m         target,\n\u001b[1;32m   3044\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   3045\u001b[0m         size_average\u001b[39m=\u001b[39;49msize_average,\n\u001b[1;32m   3046\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   3047\u001b[0m         reduce\u001b[39m=\u001b[39;49mreduce,\n\u001b[1;32m   3048\u001b[0m         reduction\u001b[39m=\u001b[39;49mreduction,\n\u001b[1;32m   3049\u001b[0m         label_smoothing\u001b[39m=\u001b[39;49mlabel_smoothing,\n\u001b[1;32m   3050\u001b[0m     )\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (228146)."
     ]
    }
   ],
   "source": [
    "#Optimized for mini batches\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "lr = -0.01\n",
    "epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    batch = torch.randint(0, inputs.shape[0], (batch_size,))\n",
    "\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "\n",
    "    embedded_feedforward = embedding[inputs[batch]].view(-1, context_size*2)\n",
    "    hidden_layer = torch.tanh(embedded_feedforward @ W1 + b1)\n",
    "    logits = hidden_layer @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, labels[batch])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for param in params:\n",
    "        param.data += lr * param.grad\n",
    "\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "print(loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
