{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    #words = text.splitlines()\n",
    "    vocab = sorted(set(\"\".join(text)))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:integer for integer, char in enumerate(vocab)}\n",
    "itos = {integer:char for integer, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda enc: [stoi[c] for c in enc]\n",
    "decode = lambda dec: \"\".join([itos[i] for i in dec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "\n",
    "train_split = int(len(text)*train_percent)\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "train_data = data[:train_split]\n",
    "test_data = data[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, context_size, batch_size):\n",
    "    batch_idx = torch.randint(0, len(data)-context_size, (batch_size,))\n",
    "    x = torch.stack([data[idx:idx+context_size] for idx in batch_idx], dim=0)\n",
    "    y = torch.stack([data[idx+1:idx+context_size+1] for idx in batch_idx], dim=0)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "context_size = 8\n",
    "batch_size = 1\n",
    "#get_batch(train_data, context_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, context_size, n_dim, vocab_size):\n",
    "        super(NGramModel, self).__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_dim)\n",
    "\n",
    "    def forward(self, x, batched):\n",
    "        logits = self.token_embedding(x)\n",
    "        #print(logits)\n",
    "        if batched == True:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "        #print(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #logits = self.token_embedding(idx)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(idx, False)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            #print(probs)\n",
    "            #print(probs.shape)\n",
    "            \n",
    "            #need a new idx to stack onto the previous idx and do the logits again but now with some context\n",
    "            #print(f\"PREV IDX --> {idx}\")\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            #print()\n",
    "            #print()\n",
    "            #print(f\"NEW IDX --> {new_idx}\")\n",
    "            idx = torch.cat((idx, new_idx), dim=1)\n",
    "            #print(f\"IDX --> {idx}\")\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([50, 53])\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = NGramModel(context_size=context_size, n_dim=vocab_size, vocab_size=vocab_size)\n",
    "\n",
    "batch_size = 1\n",
    "context_size = 2\n",
    "\n",
    "x, y = get_batch(train_data, context_size, batch_size)\n",
    "\n",
    "logits = n_gram_model(x, True)\n",
    "\n",
    "print()\n",
    "print(y.view(-1))\n",
    "\n",
    "#print(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#context_size = 2 #bigram\n",
    "context_size = 2\n",
    "n_gram_model = NGramModel(context_size=context_size, n_dim=vocab_size, vocab_size=vocab_size) #ndim is of vocab size because this is an n gram model and the labels are from 0 to vocab size\n",
    "optimizer = torch.optim.AdamW(params=n_gram_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 798.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4617, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "context_size = 8\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    x, y = get_batch(batch_size=batch_size, context_size=context_size, data=train_data)\n",
    "    #print(x)\n",
    "    logits = n_gram_model(x, True)\n",
    "    #print(logits)\n",
    "\n",
    "    #pytorch cross entropy loss expects the channels to be the second dimension while the minibatch as the first\n",
    "    loss = loss_fn(logits, y.view(-1))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[57,  1]]), tensor([[ 1, 46]]))\n",
      "tensor([[21,  1, 46, 39, 58, 43,  1, 51, 63,  1, 50, 47, 44, 43,  1, 46, 53, 61,\n",
      "          8,  0, 41, 39, 52, 53,  1, 54, 56, 43,  1, 39, 45, 57,  8,  0, 30, 10,\n",
      "          0, 59, 57, 58, 43, 39, 58, 46,  7, 50, 53, 59, 54, 57, 58, 46, 39, 58,\n",
      "         46, 47, 58, 46, 39, 56, 50, 50, 53, 59, 56, 43,  1, 57,  1, 47, 50,  1,\n",
      "         58, 46, 43, 57, 58,  1, 46, 43,  6,  1, 40, 43, 16,  1, 46,  1, 51, 39,\n",
      "         52, 43, 43, 50,  1, 46,  1, 40, 39, 51,  6,  0, 23, 21, 15, 21,  1, 53,\n",
      "          1, 39, 52, 53,  1, 51, 43]])\n",
      "tensor([21,  1, 46, 39, 58, 43,  1, 51, 63,  1, 50, 47, 44, 43,  1, 46, 53, 61,\n",
      "         8,  0, 41, 39, 52, 53,  1, 54, 56, 43,  1, 39, 45, 57,  8,  0, 30, 10,\n",
      "         0, 59, 57, 58, 43, 39, 58, 46,  7, 50, 53, 59, 54, 57, 58, 46, 39, 58,\n",
      "        46, 47, 58, 46, 39, 56, 50, 50, 53, 59, 56, 43,  1, 57,  1, 47, 50,  1,\n",
      "        58, 46, 43, 57, 58,  1, 46, 43,  6,  1, 40, 43, 16,  1, 46,  1, 51, 39,\n",
      "        52, 43, 43, 50,  1, 46,  1, 40, 39, 51,  6,  0, 23, 21, 15, 21,  1, 53,\n",
      "         1, 39, 52, 53,  1, 51, 43])\n",
      "[21, 1, 46, 39, 58, 43, 1, 51, 63, 1, 50, 47, 44, 43, 1, 46, 53, 61, 8, 0, 41, 39, 52, 53, 1, 54, 56, 43, 1, 39, 45, 57, 8, 0, 30, 10, 0, 59, 57, 58, 43, 39, 58, 46, 7, 50, 53, 59, 54, 57, 58, 46, 39, 58, 46, 47, 58, 46, 39, 56, 50, 50, 53, 59, 56, 43, 1, 57, 1, 47, 50, 1, 58, 46, 43, 57, 58, 1, 46, 43, 6, 1, 40, 43, 16, 1, 46, 1, 51, 39, 52, 43, 43, 50, 1, 46, 1, 40, 39, 51, 6, 0, 23, 21, 15, 21, 1, 53, 1, 39, 52, 53, 1, 51, 43]\n",
      "I hate my life how.\n",
      "cano pre ags.\n",
      "R:\n",
      "usteath-loupsthathitharlloure s il thest he, beD h maneel h bam,\n",
      "KICI o ano me\n"
     ]
    }
   ],
   "source": [
    "#print(torch.stack((torch.tensor(1), torch.tensor(2))))\n",
    "\n",
    "#tensor at [[0, 0]] will pluck out two times the 0th row in the embedding\n",
    "print(get_batch(batch_size=1, context_size=2, data=train_data))\n",
    "\n",
    "#this sample starts with a context of a single break line\n",
    "#sample = n_gram_model.generate(idx=torch.tensor([[0]]), max_new_tokens=10)\n",
    "#sample = n_gram_model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)\n",
    "#print(torch.zeros((1, 1)))\n",
    "\n",
    "sample = n_gram_model.generate(idx=torch.tensor([encode(\"I hate my life \")]), max_new_tokens=100)\n",
    "print(sample)\n",
    "print(sample[0])\n",
    "print(sample[0].tolist())\n",
    "print(decode(sample[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1383,  0.0087,  0.8069,  1.7460],\n",
      "        [-0.8166, -0.5755, -1.4708, -0.6814],\n",
      "        [ 0.4011,  1.7600,  0.8195, -0.0321],\n",
      "        [ 0.4064, -0.0048, -0.5751,  0.1558]])\n",
      "tensor([[ 1.1383,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.8166, -0.5755,  0.0000,  0.0000],\n",
      "        [ 0.4011,  1.7600,  0.8195,  0.0000],\n",
      "        [ 0.4064, -0.0048, -0.5751,  0.1558]])\n"
     ]
    }
   ],
   "source": [
    "#Attention\n",
    "\n",
    "test = torch.randn((4, 4))\n",
    "tril = torch.tril(test)\n",
    "print(test)\n",
    "print(tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
