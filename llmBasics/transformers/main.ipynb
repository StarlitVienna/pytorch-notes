{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-22 11:35:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.6’\n",
      "\n",
      "input.txt.6         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-01-22 11:35:18 (29.5 MB/s) - ‘input.txt.6’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device set to --> cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"Default device set to --> {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '/', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = {char:integer for integer, char in enumerate(chars)}\n",
    "itos = {integer:char for integer, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda enc: [stoi[c] for c in enc]\n",
    "decode = lambda dec: \"\".join([itos[i] for i in dec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9\n",
    "eval_percent = 0.1\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "train_split = int(len(data)*train_percent)\n",
    "\n",
    "train_data = data[:train_split]\n",
    "eval_data = data[train_split:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input tensor([51], device='cuda:0') expected --> 51\n",
      "For input tensor([51, 51], device='cuda:0') expected --> 52\n",
      "For input tensor([51, 51, 52], device='cuda:0') expected --> 15\n",
      "For input tensor([51, 51, 52, 15], device='cuda:0') expected --> 40\n",
      "For input tensor([51, 51, 52, 15, 40], device='cuda:0') expected --> 58\n",
      "For input tensor([51, 51, 52, 15, 40, 58], device='cuda:0') expected --> 48\n",
      "For input tensor([51, 51, 52, 15, 40, 58, 48], device='cuda:0') expected --> 42\n",
      "For input tensor([51, 51, 52, 15, 40, 58, 48, 42], device='cuda:0') expected --> 58\n"
     ]
    }
   ],
   "source": [
    "context_size = 8\n",
    "x = train_data[:context_size]\n",
    "y = train_data[1:context_size+1]\n",
    "\n",
    "#Doing it whis way is good so the transformer then know what to do from context being 1 up to context_size\n",
    "for t in range(context_size):\n",
    "    print(f\"For input {x[:t+1]} expected --> {y[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 51, 54, 61, 44,  1, 52, 44],\n",
      "        [59, 47, 48, 58,  1, 41, 54, 43]], device='cuda:0')\n",
      "tensor([[51, 54, 61, 44,  1, 52, 44,  6],\n",
      "        [47, 48, 58,  1, 41, 54, 43, 64]], device='cuda:0')\n",
      "Context: tensor([1], device='cuda:0') expected output --> 51\n",
      "Context: tensor([ 1, 51], device='cuda:0') expected output --> 54\n",
      "Context: tensor([ 1, 51, 54], device='cuda:0') expected output --> 61\n",
      "Context: tensor([ 1, 51, 54, 61], device='cuda:0') expected output --> 44\n",
      "Context: tensor([ 1, 51, 54, 61, 44], device='cuda:0') expected output --> 1\n",
      "Context: tensor([ 1, 51, 54, 61, 44,  1], device='cuda:0') expected output --> 52\n",
      "Context: tensor([ 1, 51, 54, 61, 44,  1, 52], device='cuda:0') expected output --> 44\n",
      "Context: tensor([ 1, 51, 54, 61, 44,  1, 52, 44], device='cuda:0') expected output --> 6\n",
      "Context: tensor([59], device='cuda:0') expected output --> 47\n",
      "Context: tensor([59, 47], device='cuda:0') expected output --> 48\n",
      "Context: tensor([59, 47, 48], device='cuda:0') expected output --> 58\n",
      "Context: tensor([59, 47, 48, 58], device='cuda:0') expected output --> 1\n",
      "Context: tensor([59, 47, 48, 58,  1], device='cuda:0') expected output --> 41\n",
      "Context: tensor([59, 47, 48, 58,  1, 41], device='cuda:0') expected output --> 54\n",
      "Context: tensor([59, 47, 48, 58,  1, 41, 54], device='cuda:0') expected output --> 43\n",
      "Context: tensor([59, 47, 48, 58,  1, 41, 54, 43], device='cuda:0') expected output --> 64\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, context_size):\n",
    "    batch_ix = torch.randint(0, len(data)-context_size, (batch_size,))\n",
    "    inputs = torch.stack([data[ix:ix+context_size]for ix in batch_ix])\n",
    "    labels = torch.stack([data[ix+1:ix+context_size+1] for ix in batch_ix])\n",
    "\n",
    "    return (inputs, labels)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "context_size = 8\n",
    "xb, yb = get_batch(train_data, batch_size, context_size)\n",
    "print(xb) # batch_size X context_size\n",
    "print(yb) # batch_size X context_size\n",
    "\n",
    "for btch in range(batch_size): #Batch dimension (B)\n",
    "    for t in range(context_size): #Time dimension (T)\n",
    "        context = xb[btch, :t+1]\n",
    "        output = yb[btch, t] #the labels tensor is already offseted in get_batch\n",
    "        print(f\"Context: {context} expected output --> {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, n_embd) # vocab_size X n_crammed_dimension (in this case 1 dimension per character, so it's 1:1)\n",
    "        #when taking the emb with some context the returned shape is n_inputs X context_size X n_embd\n",
    "        \n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        embedded = self.token_emb_table(inputs)\n",
    "        #print(embedded)\n",
    "        #print(embedded.shape) #if batched, the shape is batch_size X context_size X n_embd (B, T, n_embd)\n",
    "        #each character has n_embd associated values\n",
    "        #each context has context_size characters\n",
    "        #each batch has 4 inputs total\n",
    "\n",
    "        logits = self.lm_head(embedded) # batch_size X context_size X vocab_size (B, T, vocab_size)\n",
    "\n",
    "        #print(logits)\n",
    "        #print(logits.shape) # Batches (inputs) X Context Size X vocab_size (n_crammed_dimension in this case) Very simillar to the mlp model\n",
    "        #logits = logits.view(-1, self.vocab_size)\n",
    "        #logits = logits.view(self.vocab_size, -1)\n",
    "\n",
    "        if labels is not None:\n",
    "            B, T, C = logits.shape #Batch by time by channel\n",
    "            #For cross entropy, pytorch expects C (channels) to be the second dimension\n",
    "            logits = logits.view(B*T, C)\n",
    "            labels = labels.view(-1) #-1 will turn the shape from batch_size X context_size to batch_size * context_size\n",
    "            #print(logits.shape) # Batches (inputs) * Context Size X vocab_size\n",
    "            #print(labels.shape)\n",
    "            loss = F.cross_entropy(logits, labels) #It will compare the highest value in a logits batch to the value expected in the labels and determine a loss using negative log likelihood\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is B X T Because it's going to pluck out a number in the embedding table\n",
    "        #B --> batch\n",
    "        #T --> context\n",
    "        #or number of inputs X characters (context)\n",
    "        for i in range(max_new_tokens):\n",
    "            logits = self(idx) #B X T X C\n",
    "            logits = logits[:, -1, :] #Pluck the last character in sequence generated which is actually the new character predict (this turns the logits shape into B X C)\n",
    "            #Other way of interpreting it --> for every batch, get the last context value and it's channels\n",
    "            #print(logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            sample = torch.multinomial(probs, num_samples=1) #B X num_samples\n",
    "            idx = torch.cat((idx, sample), dim=1)\n",
    "            #print(idx)\n",
    "\n",
    "            #print(logits.shape)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4075,  0.4943,  0.1116,  ...,  0.9779,  0.1245, -0.2758],\n",
       "         [ 0.2015,  0.4193, -0.0295,  ..., -0.3305, -0.4139,  0.7940],\n",
       "         [-0.8755, -1.0539,  0.3203,  ..., -0.5948,  0.4346, -1.4882],\n",
       "         ...,\n",
       "         [ 0.3956,  0.9902, -1.0341,  ..., -1.2184,  0.5493,  0.7162],\n",
       "         [-0.3000, -0.6724,  0.4750,  ...,  0.7379,  0.3419,  0.9488],\n",
       "         [ 0.2564, -0.2031, -0.3306,  ...,  0.2243,  0.7053, -0.0562]],\n",
       "\n",
       "        [[-0.3338,  0.4826, -0.5383,  ..., -1.9343,  0.4001,  0.5283],\n",
       "         [ 0.3143, -0.2986, -0.3064,  ...,  0.1558, -0.0080,  0.1972],\n",
       "         [ 0.2015,  0.4193, -0.0295,  ..., -0.3305, -0.4139,  0.7940],\n",
       "         ...,\n",
       "         [-0.2499,  0.6423,  0.2504,  ..., -0.0449, -0.1230,  0.8345],\n",
       "         [-0.2499,  0.6423,  0.2504,  ..., -0.0449, -0.1230,  0.8345],\n",
       "         [ 0.4075,  0.4943,  0.1116,  ...,  0.9779,  0.1245, -0.2758]],\n",
       "\n",
       "        [[ 0.3143, -0.2986, -0.3064,  ...,  0.1558, -0.0080,  0.1972],\n",
       "         [ 0.2015,  0.4193, -0.0295,  ..., -0.3305, -0.4139,  0.7940],\n",
       "         [ 0.4693, -0.3451, -0.4747,  ...,  0.8803,  0.1691,  0.3719],\n",
       "         ...,\n",
       "         [-0.2499,  0.6423,  0.2504,  ..., -0.0449, -0.1230,  0.8345],\n",
       "         [-0.2499,  0.6423,  0.2504,  ..., -0.0449, -0.1230,  0.8345],\n",
       "         [ 0.2015,  0.4193, -0.0295,  ..., -0.3305, -0.4139,  0.7940]],\n",
       "\n",
       "        [[-0.0630, -0.5880, -0.7062,  ...,  0.3454,  0.0477,  0.3234],\n",
       "         [ 0.3143, -0.2986, -0.3064,  ...,  0.1558, -0.0080,  0.1972],\n",
       "         [ 0.3143, -0.2986, -0.3064,  ...,  0.1558, -0.0080,  0.1972],\n",
       "         ...,\n",
       "         [-0.3968, -0.2107,  0.0984,  ...,  1.2073, -0.5968, -0.9762],\n",
       "         [-0.7585, -0.2189,  0.3437,  ...,  0.3633,  0.7825, -1.5463],\n",
       "         [ 0.6792, -0.5785, -0.2337,  ...,  0.4517,  0.8608, -0.8291]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "context_size = 8\n",
    "model = BigramLanguageModel(vocab_size=vocab_size, n_embd=32)\n",
    "x, y = get_batch(batch_size=batch_size, context_size=context_size, data=train_data)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], device='cuda:0')\n",
      "tensor([[0]], device='cuda:0')\n",
      "\n",
      "f;LOD:.!PWNrBs\n",
      "$y/HSc;:AhOORP;yOoTVRIs'uIsDm-3:$He'I/:YorZ-uY.bbUan;M,O&CfIiNZa/b\n",
      "ZyVFF,hrz\n",
      "kO&kYDYUoo$N/J3Ruz3sj:HIJN:RmRH:DbMAYMBxy/FG/tyBPX\n",
      "Xzpnv&W;LVeXREczd,SD:kFULnBKP;\n",
      "v-,V-Vtv\n",
      " vlaDwbtkmsWbH-UKzI?-A!pBBBXclPGwaHxPXcozg.KbqcD3yFgnn$/DlCdJ$E\n",
      "b\n",
      "Wn:PnrjvPYHv'BIH EP/SORFZG$sM'OdlKvZjuzoMu-pSByIc:P$jnYvUZ,IgiC/$Ri\n",
      "dKFwfDUkPYzikUfDxtnBL3/oavOKFSXV-KD/WZE\n",
      "kjctTdWUNn!cBt!EuR:rOKD:-lweEVQHfFjZxvGOqY3Sie xqKjLqlM,:PCCxZjiSDdJUtVRDdPnBiMIVm!M'?Txwc:CTjzME mw;nyMPeHb-jikeRyelmoJpyvT!-QYr;c\n",
      "HAfDshkdq.p\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_data, batch_size, context_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(torch.zeros((1, 1)))\n",
    "print(torch.tensor([[0]]))\n",
    "sample = model.generate(torch.zeros((1, 1), dtype=torch.long), 500) #Sample from the starting character [0]\n",
    "print(decode(sample[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:03<00:00, 156.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3898, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "context_size = 8\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(data=train_data, batch_size=batch_size, context_size=context_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To tamy t out g'sshang tais gabener tsen ast be pe\n"
     ]
    }
   ],
   "source": [
    "sample = model.generate(torch.zeros((1, 1), dtype=torch.long), 50) #Sample from the starting character [0]\n",
    "print(decode(sample[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
