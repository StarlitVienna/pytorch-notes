{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device set to --> cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"Default device set to --> {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", download=True, train=True, transform=torchvision.transforms.ToTensor(), target_transform=None)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", download=True, train=False, transform=torchvision.transforms.ToTensor(), target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, generator=g, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, generator=g, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(L.LightningModule):\n",
    "    def __init__(self, n_channels):\n",
    "        super(BasicCNN, self).__init__()\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=8*8*8, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8*8*8, out_channels=8*8*8, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8*8*8, out_channels=8*8*8, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8*8*8, out_channels=8*8*8, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=0, out_features=10)\n",
    "        )\n",
    "\n",
    "        self.lr = 0.01\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor():\n",
    "        print(x.shape)\n",
    "        out1 = self.conv_block_1(x)\n",
    "        out2 = self.conv_block_2(out1)\n",
    "        #print(out2)\n",
    "        print(out2.shape)\n",
    "\n",
    "        out3 = self.classifier(out2)\n",
    "        \n",
    "\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(params=self.parameters(),lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss_fn(output, y)\n",
    "\n",
    "        self.log(\"loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([512, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evie/torching/.venv/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x49 and 0x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m BasicCNN(n_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x, y \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model(x\u001b[39m.\u001b[39;49mto(device))\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#print(out2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(out2\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m out3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(out2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x49 and 0x10)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BasicCNN(n_channels=1)\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/evie/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "tensor([[[[4.8774e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.7106e-03],\n",
      "          [3.5692e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1812e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.3404e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.3756e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5000e-02, 3.0357e-02, 3.9391e-02,  ..., 5.2493e-02,\n",
      "           4.5937e-02, 2.1850e-02],\n",
      "          [2.2222e-02, 3.8395e-02, 3.8125e-02,  ..., 3.7530e-02,\n",
      "           5.1510e-02, 1.9921e-02],\n",
      "          [2.1682e-02, 2.7438e-02, 2.0753e-02,  ..., 2.9587e-02,\n",
      "           2.9156e-02, 2.1708e-02],\n",
      "          ...,\n",
      "          [4.0055e-02, 4.9812e-02, 6.2097e-02,  ..., 4.7550e-02,\n",
      "           2.9425e-02, 1.9784e-02],\n",
      "          [1.9727e-02, 1.7728e-02, 3.5209e-02,  ..., 1.7544e-02,\n",
      "           2.7557e-02, 1.9735e-02],\n",
      "          [3.3013e-02, 2.2490e-02, 1.8424e-02,  ..., 2.2980e-02,\n",
      "           2.3585e-02, 1.9204e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4347e-02,\n",
      "           2.6825e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 9.5907e-03,  ..., 1.8341e-02,\n",
      "           1.5409e-02, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1395e-02,\n",
      "           4.9000e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 1.6656e-02, 1.6717e-02,  ..., 3.5552e-02,\n",
      "           4.3996e-06, 0.0000e+00],\n",
      "          [1.1158e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.1383e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0154e-03,\n",
      "           1.5909e-04, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.5794e-02, 1.3790e-02, 1.0747e-02,  ..., 3.1835e-02,\n",
      "           1.8015e-02, 2.3331e-02],\n",
      "          [1.1454e-02, 2.2727e-02, 6.2519e-02,  ..., 4.7611e-02,\n",
      "           3.6817e-02, 2.5821e-02],\n",
      "          [1.0735e-02, 2.7659e-02, 4.5688e-02,  ..., 4.0378e-02,\n",
      "           3.6778e-02, 2.3991e-02],\n",
      "          ...,\n",
      "          [2.6583e-02, 3.6843e-02, 3.3961e-02,  ..., 2.5699e-02,\n",
      "           3.5699e-02, 2.0273e-02],\n",
      "          [3.7049e-02, 3.7316e-02, 5.6984e-02,  ..., 2.4962e-02,\n",
      "           3.2224e-02, 2.0166e-02],\n",
      "          [1.9144e-02, 4.8925e-03, 0.0000e+00,  ..., 1.3864e-02,\n",
      "           1.5789e-02, 1.9284e-02]],\n",
      "\n",
      "         [[1.9870e-02, 2.0754e-02, 3.6583e-02,  ..., 5.5564e-02,\n",
      "           2.3394e-02, 1.2318e-02],\n",
      "          [1.6686e-02, 2.4559e-02, 5.7789e-02,  ..., 5.2325e-02,\n",
      "           5.4736e-02, 1.5678e-03],\n",
      "          [1.7319e-02, 2.4497e-02, 5.4057e-02,  ..., 5.3821e-02,\n",
      "           3.7860e-02, 6.6235e-03],\n",
      "          ...,\n",
      "          [5.7353e-02, 5.8862e-02, 4.7842e-02,  ..., 4.6574e-02,\n",
      "           1.3561e-02, 7.7346e-03],\n",
      "          [3.9963e-02, 5.5563e-02, 5.0043e-02,  ..., 3.5579e-02,\n",
      "           1.1952e-02, 1.1900e-02],\n",
      "          [9.4052e-03, 7.1349e-03, 1.7280e-02,  ..., 2.8996e-03,\n",
      "           1.8928e-05, 1.9398e-03]]],\n",
      "\n",
      "\n",
      "        [[[4.8280e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.8320e-03],\n",
      "          [1.3856e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [7.6083e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.9819e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.8600e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5759e-02, 3.3225e-02, 4.2201e-02,  ..., 3.8037e-02,\n",
      "           3.2575e-02, 2.2062e-02],\n",
      "          [3.0821e-02, 3.3413e-02, 3.2261e-02,  ..., 3.9139e-02,\n",
      "           4.3682e-02, 2.0716e-02],\n",
      "          [3.3812e-02, 2.2113e-02, 3.2602e-02,  ..., 2.4272e-02,\n",
      "           4.4378e-02, 3.0116e-02],\n",
      "          ...,\n",
      "          [2.3204e-02, 3.4691e-02, 4.6142e-02,  ..., 3.8055e-02,\n",
      "           8.1854e-03, 1.7931e-02],\n",
      "          [2.7911e-02, 3.4419e-02, 1.3419e-02,  ..., 1.5269e-02,\n",
      "           2.7124e-02, 1.8116e-02],\n",
      "          [2.2633e-02, 3.4983e-02, 3.1129e-02,  ..., 2.1507e-02,\n",
      "           2.2997e-02, 1.9922e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 4.3288e-04,  ..., 2.1992e-02,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 3.2674e-03,  ..., 0.0000e+00,\n",
      "           1.2308e-02, 5.5172e-03],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 9.3870e-03,  ..., 3.3203e-02,\n",
      "           3.2551e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 9.9668e-03,  ..., 2.5845e-02,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1082e-03, 1.3930e-03, 1.6649e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3742e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.4372e-02, 1.3795e-02, 8.5812e-03,  ..., 1.0771e-02,\n",
      "           1.5291e-02, 2.2760e-02],\n",
      "          [1.0374e-02, 3.1736e-02, 3.1927e-02,  ..., 3.4790e-02,\n",
      "           2.4601e-02, 2.2917e-02],\n",
      "          [2.7039e-02, 3.0831e-02, 3.6194e-02,  ..., 3.8182e-02,\n",
      "           2.2047e-02, 3.7050e-02],\n",
      "          ...,\n",
      "          [3.2428e-02, 2.7268e-02, 4.0085e-02,  ..., 5.8785e-02,\n",
      "           3.3473e-02, 3.3054e-02],\n",
      "          [2.0315e-02, 1.9795e-02, 3.8452e-02,  ..., 2.1839e-02,\n",
      "           1.6463e-02, 2.3924e-02],\n",
      "          [1.4093e-02, 9.5766e-03, 9.6799e-03,  ..., 1.0686e-02,\n",
      "           1.5474e-02, 1.9153e-02]],\n",
      "\n",
      "         [[2.1404e-02, 2.4696e-02, 3.6965e-02,  ..., 2.8647e-02,\n",
      "           2.7299e-02, 1.5388e-02],\n",
      "          [2.1749e-02, 4.4418e-02, 4.6995e-02,  ..., 5.2179e-02,\n",
      "           3.5744e-02, 8.6959e-03],\n",
      "          [2.9036e-02, 5.3104e-02, 4.0060e-02,  ..., 4.7574e-02,\n",
      "           4.4996e-02, 3.3148e-02],\n",
      "          ...,\n",
      "          [2.4899e-02, 3.9835e-02, 5.0460e-02,  ..., 6.8015e-02,\n",
      "           3.3295e-02, 8.4583e-03],\n",
      "          [2.1214e-02, 2.0007e-02, 3.7547e-02,  ..., 3.1971e-02,\n",
      "           3.0316e-02, 8.6226e-03],\n",
      "          [1.0668e-02, 9.1477e-03, 6.9252e-03,  ..., 1.7835e-03,\n",
      "           4.5157e-04, 1.7948e-04]]],\n",
      "\n",
      "\n",
      "        [[[4.8609e-03, 1.0282e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.8197e-03],\n",
      "          [3.5566e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.8370e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [3.1868e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.2873e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.5975e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5273e-02, 3.3945e-02, 5.3604e-02,  ..., 4.2660e-02,\n",
      "           3.2428e-02, 2.1628e-02],\n",
      "          [2.2969e-02, 3.8345e-02, 3.4419e-02,  ..., 5.0799e-02,\n",
      "           3.3382e-02, 1.9064e-02],\n",
      "          [2.3490e-02, 3.1842e-02, 2.6522e-02,  ..., 3.4156e-02,\n",
      "           3.2887e-02, 2.1448e-02],\n",
      "          ...,\n",
      "          [2.2701e-02, 3.8499e-02, 4.2544e-02,  ..., 5.0516e-02,\n",
      "           3.2539e-02, 2.0080e-02],\n",
      "          [2.2450e-02, 2.8095e-02, 2.3411e-02,  ..., 0.0000e+00,\n",
      "           2.5521e-02, 2.0936e-02],\n",
      "          [2.2140e-02, 2.7772e-02, 1.4472e-02,  ..., 2.4643e-02,\n",
      "           2.5834e-02, 1.8816e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9795e-02,\n",
      "           2.2818e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 7.2817e-04,  ..., 9.5123e-03,\n",
      "           1.0572e-02, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2036e-02,\n",
      "           6.2485e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 9.4130e-04,  ..., 3.1141e-02,\n",
      "           6.6368e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2900e-03,\n",
      "           0.0000e+00, 1.1383e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.4630e-02, 1.3289e-02, 8.8944e-03,  ..., 1.6157e-02,\n",
      "           1.6712e-02, 2.2753e-02],\n",
      "          [1.1453e-02, 3.3741e-02, 4.4623e-02,  ..., 2.9789e-02,\n",
      "           2.5951e-02, 2.1887e-02],\n",
      "          [1.0794e-02, 4.5660e-02, 2.8137e-02,  ..., 2.6828e-02,\n",
      "           2.8005e-02, 2.0166e-02],\n",
      "          ...,\n",
      "          [1.0777e-02, 3.6564e-02, 4.4704e-02,  ..., 3.6132e-02,\n",
      "           3.3295e-02, 2.0166e-02],\n",
      "          [1.0735e-02, 2.8487e-02, 4.1681e-02,  ..., 2.1630e-02,\n",
      "           2.4056e-02, 2.0166e-02],\n",
      "          [1.4675e-02, 2.4436e-02, 2.4051e-02,  ..., 1.8881e-02,\n",
      "           1.5588e-02, 1.9284e-02]],\n",
      "\n",
      "         [[2.0836e-02, 2.3409e-02, 3.6570e-02,  ..., 3.1396e-02,\n",
      "           1.6884e-02, 1.5572e-02],\n",
      "          [1.7184e-02, 3.6042e-02, 5.9449e-02,  ..., 4.6354e-02,\n",
      "           2.1106e-02, 8.6695e-03],\n",
      "          [1.9926e-02, 2.5503e-02, 6.6002e-02,  ..., 5.8049e-02,\n",
      "           3.1531e-02, 1.0169e-02],\n",
      "          ...,\n",
      "          [1.7570e-02, 3.5582e-02, 6.2879e-02,  ..., 4.3499e-02,\n",
      "           2.9020e-02, 7.7447e-03],\n",
      "          [1.8558e-02, 2.6885e-02, 6.6778e-02,  ..., 2.7274e-02,\n",
      "           1.1062e-02, 1.2211e-02],\n",
      "          [8.0752e-03, 1.1794e-02, 1.8792e-02,  ..., 1.1912e-02,\n",
      "           0.0000e+00, 1.8601e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[4.4563e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.8342e-03],\n",
      "          [2.1395e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [7.9800e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.9902e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.6902e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.9417e-02, 4.1349e-02, 5.4958e-02,  ..., 4.3602e-02,\n",
      "           3.4198e-02, 2.2433e-02],\n",
      "          [3.1127e-02, 4.3085e-02, 4.9647e-02,  ..., 5.3166e-02,\n",
      "           4.2495e-02, 2.0647e-02],\n",
      "          [3.2540e-02, 4.0156e-02, 5.5550e-02,  ..., 3.1885e-02,\n",
      "           4.0386e-02, 2.1197e-02],\n",
      "          ...,\n",
      "          [2.4558e-02, 3.3202e-02, 2.5672e-02,  ..., 2.4741e-02,\n",
      "           4.5876e-02, 2.1607e-02],\n",
      "          [2.8309e-02, 2.7572e-02, 3.6375e-02,  ..., 2.4890e-02,\n",
      "           2.4808e-02, 2.0373e-02],\n",
      "          [2.0300e-02, 2.9596e-02, 2.1899e-02,  ..., 1.6860e-02,\n",
      "           2.5163e-02, 1.9068e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.0231e-02,  ..., 2.7036e-02,\n",
      "           1.5953e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 4.1786e-03,  ..., 2.1818e-02,\n",
      "           1.7326e-02, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 1.3597e-02,  ..., 0.0000e+00,\n",
      "           1.5341e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 4.6861e-03,  ..., 6.1361e-02,\n",
      "           6.1956e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.9036e-03,\n",
      "           0.0000e+00, 1.8903e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4789e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.4228e-02, 1.1821e-02, 7.4936e-03,  ..., 1.5211e-02,\n",
      "           1.4533e-02, 2.2767e-02],\n",
      "          [1.7824e-02, 4.5337e-02, 5.1108e-02,  ..., 3.9314e-02,\n",
      "           3.2550e-02, 2.2404e-02],\n",
      "          [2.2188e-02, 3.6344e-02, 1.3610e-02,  ..., 2.7810e-02,\n",
      "           2.7674e-02, 2.0610e-02],\n",
      "          ...,\n",
      "          [2.2203e-02, 4.9108e-02, 2.6248e-02,  ..., 2.5953e-02,\n",
      "           3.8165e-02, 2.5772e-02],\n",
      "          [1.2149e-02, 2.4276e-02, 4.2514e-02,  ..., 5.2612e-02,\n",
      "           3.6067e-02, 2.1961e-02],\n",
      "          [1.4604e-02, 2.3355e-02, 6.2903e-03,  ..., 5.6195e-03,\n",
      "           1.1121e-02, 2.0140e-02]],\n",
      "\n",
      "         [[2.0955e-02, 3.1612e-02, 4.4164e-02,  ..., 2.9830e-02,\n",
      "           1.9878e-02, 1.4863e-02],\n",
      "          [1.9959e-02, 4.7141e-02, 5.1325e-02,  ..., 4.3716e-02,\n",
      "           3.6482e-02, 5.4431e-03],\n",
      "          [2.4471e-02, 4.5276e-02, 6.3034e-02,  ..., 5.3503e-02,\n",
      "           3.9625e-02, 6.0659e-03],\n",
      "          ...,\n",
      "          [1.9797e-02, 4.6190e-02, 6.2905e-02,  ..., 5.4701e-02,\n",
      "           3.8377e-02, 4.7911e-03],\n",
      "          [2.3641e-02, 2.7778e-02, 5.1784e-02,  ..., 3.7726e-02,\n",
      "           2.7918e-02, 6.0928e-03],\n",
      "          [9.2475e-03, 1.0883e-02, 1.3737e-02,  ..., 1.9889e-02,\n",
      "           2.6296e-03, 1.0782e-03]]],\n",
      "\n",
      "\n",
      "        [[[4.8774e-03, 1.0544e-03, 1.0394e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.8197e-03],\n",
      "          [3.5692e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1812e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.8671e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.0881e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.6753e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.5000e-02, 2.9362e-02, 3.5057e-02,  ..., 4.4772e-02,\n",
      "           3.1898e-02, 2.1569e-02],\n",
      "          [2.1849e-02, 2.7642e-02, 3.3930e-02,  ..., 4.4125e-02,\n",
      "           3.6337e-02, 2.0457e-02],\n",
      "          [2.2568e-02, 3.5148e-02, 3.3232e-02,  ..., 1.6245e-02,\n",
      "           3.6559e-02, 1.9307e-02],\n",
      "          ...,\n",
      "          [2.6617e-02, 3.9209e-02, 4.4296e-02,  ..., 1.6471e-02,\n",
      "           2.2144e-02, 1.8584e-02],\n",
      "          [2.6789e-02, 2.7909e-02, 1.6867e-02,  ..., 2.5395e-02,\n",
      "           2.2843e-02, 1.7807e-02],\n",
      "          [2.3139e-02, 3.1714e-02, 2.4814e-02,  ..., 2.6862e-02,\n",
      "           2.4711e-02, 1.9876e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3525e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0607e-02,\n",
      "           8.1216e-03, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           8.7093e-03, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 8.3910e-04, 6.2143e-03,  ..., 2.7522e-02,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 5.1665e-03,  ..., 9.8107e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.1383e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6590e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.5794e-02, 1.4358e-02, 1.3482e-02,  ..., 2.4170e-02,\n",
      "           2.4063e-02, 2.2753e-02],\n",
      "          [1.1454e-02, 1.1622e-02, 2.9524e-02,  ..., 3.2255e-02,\n",
      "           2.6716e-02, 2.2233e-02],\n",
      "          [1.0735e-02, 1.9186e-02, 3.8604e-02,  ..., 3.6259e-02,\n",
      "           2.6556e-02, 2.1937e-02],\n",
      "          ...,\n",
      "          [1.0646e-02, 4.1770e-02, 5.1904e-02,  ..., 4.6398e-02,\n",
      "           2.4568e-02, 2.1016e-02],\n",
      "          [1.3379e-02, 3.3207e-02, 3.3824e-02,  ..., 1.6679e-02,\n",
      "           1.3196e-02, 2.0263e-02],\n",
      "          [1.3539e-02, 1.6856e-02, 4.8339e-03,  ..., 1.4699e-02,\n",
      "           1.3936e-02, 1.9284e-02]],\n",
      "\n",
      "         [[1.9689e-02, 1.9115e-02, 2.4618e-02,  ..., 5.0882e-02,\n",
      "           1.1989e-02, 1.5122e-02],\n",
      "          [1.6356e-02, 1.8994e-02, 3.4661e-02,  ..., 4.9769e-02,\n",
      "           4.4894e-02, 6.2160e-03],\n",
      "          [1.7546e-02, 2.2596e-02, 5.8065e-02,  ..., 3.2207e-02,\n",
      "           4.4066e-02, 4.5988e-03],\n",
      "          ...,\n",
      "          [2.0382e-02, 3.6477e-02, 6.0546e-02,  ..., 3.3460e-02,\n",
      "           2.2738e-02, 9.0542e-03],\n",
      "          [2.1732e-02, 2.6254e-02, 4.7155e-02,  ..., 2.1151e-02,\n",
      "           1.2274e-02, 1.0910e-02],\n",
      "          [7.7240e-03, 9.0894e-03, 4.0038e-03,  ..., 1.4138e-03,\n",
      "           1.1756e-03, 2.7300e-03]]],\n",
      "\n",
      "\n",
      "        [[[4.8774e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.8197e-03],\n",
      "          [3.5692e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1812e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [3.1812e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.2873e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.5975e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.4946e-02, 3.3095e-02, 4.3679e-02,  ..., 4.3066e-02,\n",
      "           3.1533e-02, 2.2694e-02],\n",
      "          [2.1958e-02, 3.2015e-02, 2.4989e-02,  ..., 4.0865e-02,\n",
      "           3.8464e-02, 1.8986e-02],\n",
      "          [2.1020e-02, 2.8544e-02, 3.4047e-02,  ..., 9.2032e-03,\n",
      "           2.6565e-02, 1.9517e-02],\n",
      "          ...,\n",
      "          [2.1040e-02, 3.3875e-02, 3.7248e-02,  ..., 3.4846e-02,\n",
      "           2.5616e-02, 1.7211e-02],\n",
      "          [2.1248e-02, 2.8528e-02, 1.4918e-02,  ..., 1.8360e-02,\n",
      "           2.6505e-02, 1.7636e-02],\n",
      "          [2.0182e-02, 3.3944e-02, 2.6115e-02,  ..., 2.6164e-02,\n",
      "           2.5750e-02, 1.9950e-02]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6935e-02,\n",
      "           1.8227e-02, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7593e-03,\n",
      "           1.1201e-02, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4404e-02,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 1.0270e-02,  ..., 9.9845e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.1383e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.2260e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.5728e-02, 1.2875e-02, 2.7258e-02,  ..., 2.8335e-02,\n",
      "           1.8839e-02, 2.2753e-02],\n",
      "          [1.1454e-02, 1.8735e-02, 3.7065e-02,  ..., 3.6632e-02,\n",
      "           2.8932e-02, 2.1887e-02],\n",
      "          [1.0735e-02, 2.0333e-02, 2.8367e-02,  ..., 3.7969e-02,\n",
      "           2.3901e-02, 2.0166e-02],\n",
      "          ...,\n",
      "          [1.0735e-02, 2.3269e-02, 3.8253e-02,  ..., 3.3383e-02,\n",
      "           2.1592e-02, 2.0166e-02],\n",
      "          [1.0735e-02, 2.6115e-02, 4.2436e-02,  ..., 1.8381e-02,\n",
      "           1.8627e-02, 2.0166e-02],\n",
      "          [1.4675e-02, 2.3672e-02, 1.1586e-02,  ..., 1.1722e-02,\n",
      "           1.5371e-02, 1.9284e-02]],\n",
      "\n",
      "         [[1.9688e-02, 2.2753e-02, 5.5273e-02,  ..., 3.1728e-02,\n",
      "           1.9353e-02, 1.4053e-02],\n",
      "          [1.6503e-02, 2.0660e-02, 4.6837e-02,  ..., 5.5318e-02,\n",
      "           4.0536e-02, 6.7879e-03],\n",
      "          [1.6279e-02, 2.0313e-02, 4.3171e-02,  ..., 4.2965e-02,\n",
      "           2.5346e-02, 9.5392e-03],\n",
      "          ...,\n",
      "          [1.6215e-02, 2.6821e-02, 5.8740e-02,  ..., 4.0363e-02,\n",
      "           3.2769e-03, 1.2097e-02],\n",
      "          [1.6554e-02, 1.9783e-02, 5.5692e-02,  ..., 2.7938e-02,\n",
      "           1.4165e-02, 1.2105e-02],\n",
      "          [5.7783e-03, 6.5527e-03, 1.1958e-02,  ..., 8.5239e-03,\n",
      "           2.0495e-03, 2.6325e-03]]]], device='cuda:0',\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "torch.Size([32, 512, 7, 7])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tuner \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mpytorch\u001b[39m.\u001b[39mtuner\u001b[39m.\u001b[39mTuner(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m find_lr \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mlr_find(model, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader, min_lr\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m, max_lr\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, early_stop_threshold\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m new_lr \u001b[39m=\u001b[39m find_lr\u001b[39m.\u001b[39msuggestion()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mlr \u001b[39m=\u001b[39m new_lr\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:177\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m lr_finder_callback\u001b[39m.\u001b[39m_early_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m [lr_finder_callback] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\u001b[39m.\u001b[39mcallbacks\n\u001b[0;32m--> 177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[1;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m [cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\u001b[39m.\u001b[39mcallbacks \u001b[39mif\u001b[39;00m cb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lr_finder_callback]\n\u001b[1;32m    181\u001b[0m \u001b[39mreturn\u001b[39;00m lr_finder_callback\u001b[39m.\u001b[39moptimal_lr\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:969\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[0;32m--> 969\u001b[0m     call\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mon_fit_start\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    970\u001b[0m     call\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    972\u001b[0m _log_hyperparams(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             fn(trainer, trainer\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:126\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_fit_start\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_find(trainer, pl_module)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:110\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlr_find\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 110\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lr \u001b[39m=\u001b[39m _lr_find(\n\u001b[1;32m    111\u001b[0m             trainer,\n\u001b[1;32m    112\u001b[0m             pl_module,\n\u001b[1;32m    113\u001b[0m             min_lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_lr,\n\u001b[1;32m    114\u001b[0m             max_lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_lr,\n\u001b[1;32m    115\u001b[0m             num_training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_training_steps,\n\u001b[1;32m    116\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mode,\n\u001b[1;32m    117\u001b[0m             early_stop_threshold\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_stop_threshold,\n\u001b[1;32m    118\u001b[0m             update_attr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_attr,\n\u001b[1;32m    119\u001b[0m             attr_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attr_name,\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_early_exit:\n\u001b[1;32m    123\u001b[0m         \u001b[39mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:275\u001b[0m, in \u001b[0;36m_lr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    272\u001b[0m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer)\n\u001b[1;32m    274\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m _try_loop_run(trainer, params)\n\u001b[1;32m    277\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training \u001b[39m+\u001b[39m start_steps:\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:515\u001b[0m, in \u001b[0;36m_try_loop_run\u001b[0;34m(trainer, params)\u001b[0m\n\u001b[1;32m    513\u001b[0m loop\u001b[39m.\u001b[39mload_state_dict(deepcopy(params[\u001b[39m\"\u001b[39m\u001b[39mloop_state_dict\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m    514\u001b[0m loop\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    189\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    266\u001b[0m     trainer,\n\u001b[1;32m    267\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    269\u001b[0m     batch_idx,\n\u001b[1;32m    270\u001b[0m     optimizer,\n\u001b[1;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 161\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    164\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn(output, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evie/torching/imaging/conv/fashion/cnnstyle.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3039\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m \n\u001b[1;32m   2975\u001b[0m \u001b[39mSee :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39m    >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target, weight):\n\u001b[0;32m-> 3039\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3040\u001b[0m         cross_entropy,\n\u001b[1;32m   3041\u001b[0m         (\u001b[39minput\u001b[39;49m, target, weight),\n\u001b[1;32m   3042\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3043\u001b[0m         target,\n\u001b[1;32m   3044\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   3045\u001b[0m         size_average\u001b[39m=\u001b[39;49msize_average,\n\u001b[1;32m   3046\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   3047\u001b[0m         reduce\u001b[39m=\u001b[39;49mreduce,\n\u001b[1;32m   3048\u001b[0m         reduction\u001b[39m=\u001b[39;49mreduction,\n\u001b[1;32m   3049\u001b[0m         label_smoothing\u001b[39m=\u001b[39;49mlabel_smoothing,\n\u001b[1;32m   3050\u001b[0m     )\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1561\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/torching/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = L.Trainer(max_epochs=1,accelerator=\"gpu\")\n",
    "tuner = L.pytorch.tuner.Tuner(trainer=trainer)\n",
    "\n",
    "find_lr = tuner.lr_find(model, train_dataloaders=train_dataloader, min_lr=0.0001, max_lr=1.0, early_stop_threshold=None)\n",
    "new_lr = find_lr.suggestion()\n",
    "model.lr = new_lr\n",
    "\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader)\n",
    "#out = model(next(iter(train_dataloader))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n",
      "Predicted --> 9 - nine\n",
      "Expected --> 4 - four\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    #x, y = next(iter(train_dataloader))\n",
    "    x, y = train_dataset[987]\n",
    "\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "print(preds)\n",
    "print(f\"Predicted --> {train_dataset.classes[preds]}\")\n",
    "print(f\"Expected --> {train_dataset.classes[y]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
