{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device set to cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"Default device set to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '/', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    vocab = sorted(set(text))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:integer for integer, char in enumerate(vocab)}\n",
    "itos = {integer:char for integer, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda enc: [stoi[c] for c in enc]\n",
    "decode = lambda dec: \"\".join([itos[i] for i in dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "vocab_size = len(stoi)\n",
    "n_embd = 32\n",
    "block_size = 128\n",
    "head_size = 256\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, context_size, batch_size):\n",
    "    batch_idx = torch.randint(0, len(data)-context_size, (batch_size,)) #need to be idx before len(data)-context_Size cuz if there ins't enough number after the random idx, it will throw an error\n",
    "    x = torch.stack([data[idx:idx+context_size] for idx in batch_idx], dim=0)\n",
    "    y = torch.stack([data[idx+1:idx+1+context_size] for idx in batch_idx], dim=0)\n",
    "\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[44,  1, 45, 54, 57,  1, 54, 53]], device='cuda:0')\n",
      "tensor([[ 1, 45, 54, 57,  1, 54, 53, 44]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(data, 8, 1)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(in_features=n_embd, out_features=head_size)\n",
    "        self.key = nn.Linear(in_features=n_embd, out_features=head_size)\n",
    "        self.value = nn.Linear(in_features=n_embd, out_features=head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "\n",
    "        #print(f\"q.shape --> {q.shape}\")\n",
    "        #print(f\"k.shape --> {k.shape}\")\n",
    "\n",
    "        score_matrix = q @ (k.transpose(-2, -1))\n",
    "        #print(f\"score_matrix.shape --> {score_matrix.shape}\") # --> B, T, T #how much each word related to one another\n",
    "        \n",
    "        scaled_matrix = score_matrix * k.shape[-1] ** -0.5 # == score_matrix / k.shape[-1]**0.5\n",
    "        #print(f\"scaled_matrix.shape --> {scaled_matrix.shape}\") # --> B, T, T\n",
    "\n",
    "        attention_weights = torch.softmax(scaled_matrix, dim=-1)\n",
    "        #print(f\"attention_weights.shape --> {attention_weights.shape}\")\n",
    "\n",
    "        v = self.value(x)\n",
    "        #print(f\"v.shape -->\")\n",
    "        output = attention_weights @ v # B, T, T @ B, T, hs --> B, T, hs\n",
    "        #print(f\"output.shape --> {output.shape}\")\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.heads = [Head(head_size=head_size) for _ in range(n_heads)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        head_outputs = torch.cat([h(x) for h in self.heads])\n",
    "        #print(head_outputs.shape) # --> T, T, C\n",
    "        return head_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_token_emb = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.head = Head(head_size=head_size)\n",
    "        self.multihead = MultiHeadAttention(head_size=head_size, n_heads=4)\n",
    "\n",
    "        self.lm_head = nn.Linear(in_features=head_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        embedded_tokens = self.token_emb(x)\n",
    "        pos_embedded = self.positional_token_emb(torch.arange(start=0, end=T, step=1))\n",
    "\n",
    "        result_embedded = embedded_tokens + pos_embedded\n",
    "        #print(result_embedded.shape)\n",
    "\n",
    "        #Encoder\n",
    "        logits = self.head(result_embedded)\n",
    "        #print(logits.shape, \"aaa\")\n",
    "        logits = self.lm_head(logits)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            labels = tagets.view(B*T)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            return (logits, loss)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, y = get_batch(data, batch_size=32, context_size=block_size)\n",
    "print(y.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 66])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "logits = model(x, y)\n",
    "#print(logits.shape)\n",
    "#print(y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096, 66])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x, y = get_batch(data, batch_size=32, context_size=block_size)\n",
    "    logits = model(x, y)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    labels = y.view(-1)\n",
    "    #print(logits.shape)\n",
    "    #print(labels.shape)\n",
    "    #loss = F.cross_entropy(logits, labels)\n",
    "    #optimizer.zero_grad(set_to_none=True)\n",
    "    #loss.backward()\n",
    "    #optimizer.step()\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
