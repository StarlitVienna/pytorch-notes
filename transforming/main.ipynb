{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default device set to --> cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"default device set to --> {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '/', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    vocab = sorted(set(text))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:integer for integer, char in enumerate(vocab)}\n",
    "itos = {integer:char for integer, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda enc: [stoi[c] for c in enc]\n",
    "decode = lambda dec: \"\".join([itos[i] for i in dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1, 3])\n",
      "tensor([[22,  5, 51]], device='cuda:0') tensor([[ 5, 51, 51]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, context_size):\n",
    "    batch_idx = torch.randint(0, len(data)-context_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[idx:idx+context_size] for idx in batch_idx])\n",
    "    y = torch.stack([data[idx+1:idx+1+context_size] for idx in batch_idx])\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "x, y = get_batch(data, 1, 3)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, context_size):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        self.key = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
    "        self.query = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
    "        self.value = nn.Linear(in_features=n_embd, out_features=head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "\n",
    "        k_transpose = K.transpose(-2, -1)\n",
    "\n",
    "        score_matrix = Q @ k_transpose\n",
    "        scaled_matrix = score_matrix * (1/(K.shape[-1]**0.5)) #K.shape[-1] == head_size\n",
    "        softmaxed_matrix = torch.softmax(scaled_matrix, dim=-1) #--> also called the attention weights\n",
    "        #wei = Q @ k_transpose * K.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        #wei = F.softmax(scaled_matrix, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        V = self.value(x) # (B,T,hs)\n",
    "        output = softmaxed_matrix @ V #--> need to be in this order because of the shapes\n",
    "        #out = softmaxed_matrix @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        heads = [Head(head_size, n_embd) for h in range(n_heads)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, context_size):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.input_token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_enc_emb = nn.Embedding(context_size, n_embd) #context size because that is the number of words in each input of x\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "        self.head = Head(head_size=512, n_embd=n_embd, context_size=context_size)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets=None, batched=True):\n",
    "\n",
    "        if batched:\n",
    "            B, T = x.shape #--> both targets and inputs are of B X T shape\n",
    "            n_words = x.shape[1]\n",
    "        else:\n",
    "            n_words = x.shape[0]\n",
    "\n",
    "        tokens_emb = self.input_token_emb(x) #--> turns into B X T X C\n",
    "        #1 positional encoding for each characters in this (because I am using a character level tokenizer)\n",
    "\n",
    "        #very simple positional encoding by just counting\n",
    "        positional_enc_idx = torch.arange(start=0, end=n_words, step=1) #--> 1 X n_words (1 X T)\n",
    "        #positional_enc = torch.arange(n_words) --> same as above\n",
    "        \n",
    "        positional_enc = self.positional_enc_emb(positional_enc_idx) #--> T X C\n",
    "        \n",
    "        result_emb = tokens_emb + positional_enc\n",
    "\n",
    "        #Encoder layer\n",
    "        head_out = self.head(result_emb)\n",
    "\n",
    "        logits = self.lm_head(head_out)\n",
    "\n",
    "        if targets is not None:\n",
    "            #print(f\"logits shape --> {logits.shape}\")\n",
    "            #print(f\"labels shape --> {targets.shape}\")\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #cross entropy loss expects targets as the second dim\n",
    "            #targets = targets.view(B*T)\n",
    "\n",
    "            #print(logits.shape)\n",
    "\n",
    "            #loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            return (logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def generate(self, starting_idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #crop starting_idx to fit in the block size (context size)\n",
    "            starting_idx = starting_idx[:, -context_size:] #predictions are only going to be made based on the last context block\n",
    "            logits = self(starting_idx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            sample = torch.multinomial(probs.view(-1, vocab_size), num_samples=1)\n",
    "            #print(starting_idx.shape)\n",
    "            #print(sample.shape)\n",
    "            starting_idx = torch.cat((starting_idx, sample), dim=1)\n",
    "        return starting_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Test for self attention\\n## only a single sentence\\nx = x[0]\\ny = y[0]\\n\\nprint(x.shape)\\nprint(y.shape)\\n'"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 512\n",
    "context_size = 256\n",
    "batch_size = 32\n",
    "\n",
    "x, y = get_batch(data=data, batch_size=batch_size, context_size=context_size)\n",
    "\n",
    "model = GPT(vocab_size=vocab_size, n_embd=n_embd, context_size=context_size)\n",
    "\n",
    "#model(x)\n",
    "\n",
    "\"\"\"\n",
    "#Test for self attention\n",
    "## only a single sentence\n",
    "x = x[0]\n",
    "y = y[0]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\"\"\"\n",
    "\n",
    "#model(x=x, targets=y, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X --> tensor([[ 1, 42, 40,  ..., 44,  1, 47],\n",
      "        [21, 14, 31,  ..., 51, 44, 44],\n",
      "        [60, 51,  1,  ..., 64,  1, 43],\n",
      "        ...,\n",
      "        [40, 53, 43,  ..., 57,  1, 54],\n",
      "        [43,  1, 50,  ..., 40, 59,  1],\n",
      "        [47, 44, 48,  ..., 44, 53,  1]], device='cuda:0')\n",
      "Y --> tensor([[42, 40, 52,  ...,  1, 47, 44],\n",
      "        [14, 31, 17,  ..., 44, 44, 43],\n",
      "        [51,  1, 45,  ...,  1, 43, 54],\n",
      "        ...,\n",
      "        [53, 43,  1,  ...,  1, 54, 45],\n",
      "        [ 1, 50, 53,  ..., 59,  1, 48],\n",
      "        [44, 48, 57,  ..., 53,  1, 22]], device='cuda:0')\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "batch_size = 32\n",
    "context_size = 256\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    x, y = get_batch(data=data, batch_size=batch_size, context_size=context_size)\n",
    "    print(f\"X --> {x}\")\n",
    "    print(f\"Y --> {y}\")\n",
    "    logits = model(x=x, targets=y, batched=True)\n",
    "    #print(logits)\n",
    "    loss = F.cross_entropy(logits, y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    preds = model.generate(starting_idx=torch.zeros((1,1),dtype=torch.long), max_new_tokens=128)\n",
    "\n",
    "\n",
    "print(preds[0].tolist())\n",
    "print(decode(preds[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
