{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model=\"mistral\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To run a Large Language Model (LLM) using Python and Hugging Face\\'s Transformers library with your GPU, follow these steps:\\n\\n1. First, make sure you have the following prerequisites:\\n   - A powerful GPU with sufficient memory to accommodate large models.\\n   - An Nvidia CUDA-enabled GPU driver installed. You can check this by running `nvidia-smi` in your terminal or command prompt.\\n   - Python (preferably 3.x) and the necessary libraries installed: Transformers, Torch, and TensorFlow (if you plan to use them interchangeably). You can install these using pip:\\n     ```\\n     pip install torch transformers\\n     ```\\n   - Make sure your GPU is recognized by Python; follow the instructions in the official PyTorch documentation to configure it.\\n\\n2. Clone the Hugging Face Transformers repository and download a pre-trained LLM model:\\n   ```bash\\n   git clone https://github.com/huggingface/transformers.git\\n   cd transformers\\n   # Download a specific pre-trained model or use an existing one (e.g., BERT, RoBERTa)\\n   wget https://huggingface.co/models/model_name/resolve/main/config.json\\n   wget https://huggingface.co/models/model_name/resolve/main/pytorch_model.bin\\n   wget https://huggingface.co/models/model_name/resolve/main/tokenizer_model.bin\\n   ```\\n\\n3. Now, let\\'s write a Python script to load the model and use it for inference:\\n\\n   ```python\\n   import torch\\n   from transformers import AutoTokenizer, AutoModelForMaskedLM\\n\\n   # Set device to GPU if available\\n   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n   # Load the tokenizer and model\\n   tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\\n   model = AutoModelForMaskedLM.from_pretrained(\"model_name\").to(device)\\n\\n   # Sample input text for demonstration purposes\\n   context = \"The quick brown fox jumps over the lazy dog.\"\\n   input_text = \"[mask]\"\\n   inputs = tokenizer.encode_plus(context, [input_text], max_length=len(context) + len(input_text), return_tensors=\"pt\").to(device)\\n\\n   # Perform the prediction\\n   outputs = model(inputs)[0][:, 0, :]\\n\\n   # Decode the predicted token id back to text\\n   predicted_token = tokenizer.decode(outputs[0].argmax().item(), skip_special_tokens=True)\\n   print(\"Predicted token:\", predicted_token)\\n   ```\\n\\nReplace \"model_name\" with the actual name of your chosen pre-trained model (e.g., \"bert-base-cased\", \"roberta-base\"). This script loads a masked language model and performs an inference task on a single input text containing a [mask] token. The predicted token is then decoded back to its original text form.\\n\\nRun the script using:\\n```bash\\npython script_name.py\\n```'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"how can I run a llm model using python and langchain using my gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
